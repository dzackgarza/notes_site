# Thursday January 30th

## Distributions

Example 1: 
Binomial Distribution

The state space is $S = \theset{0, 1, \cdots, n}$, and $P(X = i) = {n\choose i} p^j (1-p)^{n-j}$ for $0 < p < 1$.

> Note that flipping a coin once is a Bernoulli distribution, i.e. if $X$ is the number of heads in 1 flip, then $P(X=1) = p,~P(X=0) = 1-p$.

The distribution function is given by $F_X(x) = P(X < x)$ is a step function:

![Image](figures/2020-01-30-09:40.png)\

Note that this is left-continuous.

Example 2:
Gaussian (Normal) Distribution

This is given by $P(a < X < b) = \int_a^b \phi(t) ~dt$ where the density $\phi(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. 
The distribution function is given by $F_X(x) = P(X < x) = \int_{-\infty}^x \phi(t) ~dt$.

Consider the two experiments

| Flip a Coin | Roll a die |
| ----- | ----- |
|$\Omega = \theset{H, T}$ | $\Omega = [6]$ |
| $X_1 = 1$ iff $H$, $0$ iff $T$ | $X_2 = 1$ iff odd, $0$ iff even |

Note that $X_1, X_2$ have the same distribution, i.e. $P(X_1 \in A) = P(X_2 \in A)$ (??)

**Theorem:**
If $X_i, Y_i$ are two vectors of random variables with identical distribution functions, so $F_{X_i\cdots}(x_1, \cdots) = F_{Y_i \cdots}(y_1, \cdots)$,
then for all finite Borel functions $g$, $X = g(X_i, \cdots), Y = g(Y_i, \cdots)$ have identical distribution functions.

*Proof:*

1. $X$ and $Y$ are random variables, so this is clear. (?)

2. Letting $\mcg = \theset{B \in \mcb^n \suchthat P((X_i, \cdots) \in B) = P((X_i, \cdots) \in B) }$.
  Let $\mcd = \theset{\prod [-\infty, c_j) \suchthat c_j \in \RR }$, then $\mcd \subset \mcg$.
  It is also true that $\mcg$ is a $\lambda\dash$class (check).

Thus $\mcg \supset \lambda(\mcd)$, the $\lambda\dash$class generated by $\mcd$, and $\lambda(\mcd) = \sigma(\mcd)$, so $\mcd$ is a $\pi\dash$class.
Thus $P(g(x_i, \cdots) < \lambda) = P((X_i, \cdots) \in g\inv(-\infty, \lambda) (\in \mcb^n) ) = P( (Y_i, \cdots) \in g\inv[-\infty, \lambda)) = P(g(Y_i, \cdots) < \lambda)$.

$\qed$

## Chapter 3: Independence

Note that in the previous experiment, the two outcomes were independent, so $P(X_1 = 1, X_2 = 0) = P(X_1 = 1) P(X_2 = 2)$.
We can take this as the definition of independence.
Alternative characterization: $P(F_2 \mid F_1) = P(F_1 F_2) / P(F_1) = P(F_2)$.

Similarly, if 3 events are independent, but the condition $P(F_1 F_2 F_3) = P(F_1) P(F_2) P(F_3)$ is not sufficient to capture this notion.
We additionally need pairwise independence.

**Definition:**

1. A set of random variables $X_i$ are independent iff for all $A_i \in \sigma(X_i)$, $P(A_1, \cdots, A_n) = P(A_1) \cdots P(A_n)$.

2. The classes $\mcg_i$ are independent iff for all $A_i \in \mcg_i$, $P(A_1 \cdots A_n) = P(A_1) \cdots P(A_n)$.

3. The events $F_i$ are independent iff $I_{F_i}$ are independent (for $I$ indicator functions).

*Example:*
Let $n=3$, then $\sigma(I_{F_i}) = \theset{\emptyset,\Omega, F_i, F_i^c}$.
Then since $F_i \in \sigma(F_i)$, we have $P(F_1 F_2 F_3) = P(F_1) P(F_2) P(F_3)$.
We also have $P(F_1 F_2) = P(F_1 F_2 \Omega) = P(F_1) P(F_2) P(\Omega)$, so this recovers the pairwise independence.

A sequence of random variables is iid iff they are independent and identically distributed.

Recall that the event "$A_n$ happens infinitely often" is given by $A \definedas \limsup A_n = \intersect_k \union^k A_n$.

**Theorem (The Borel 0-1 Criterion):**
If $A_n$ are independent events, then $P(\limsup A_n) = 1$ iff $\sum P(A_n) = \infty$ and 0 otherwise.

Proof:
If $\sum P(A_n) < \infty$, then $P(A) = P(\limsup A_n) \leq P(\union_k A_n) \leq \sum_k P(A_n) \converges{k\to\infty}\to 0$.
Note that we don't need independence here.

Similarly, we want to show $P(A^c) = P(\liminf A_n^c) = 0$.
Thus it suffices to show that $P(\intersect_k A_n^c) = 0$ for all $k$.
We have 

\begin{align*}
\sum_{k=1}^\infty P(A_k)P(\intersect_{n=k+1}^\infty A_n^c) &\leq
\sum_{k=1}^\infty P(A_k)P(\intersect_{n=k+1}^m A_n^c) \\
&= \sum_{k=1}^m P(A_k \intersect_{n=k+1}^m A_n^c ) \quad\text{since } A_k \intersect \intersect_{n=k+1}^m A_n^c = \emptyset \text{ by independence } \\
&= = P(\union_{k=1}^m A_k) \leq 1
.\end{align*}

Therefore,

\begin{align*}
\union_{i=1}^m = A_1(A_2^c \cdots A_m^c) \union A_2(A_3^c \cdots A_m^c) \union \cdots \union A_m
.\end{align*}

E.g. for $m=2$, we have $A_1 \union A_2 = A_1 A_2^c \union A_2$.

Letting $b_k = \intersect_{n=k}^\infty A_n^c$, we have $b_k$ monotone increasing, and since $\sum P(A_n)= \infty$ and $\sum P(A_k)b_{k+1} \leq 1$, we must have $b_k = 0$ for all $k$.
This follows because

\begin{align*}
1 \geq \sum_{k=n}^\infty P(A_k) b_{k+1} \geq b_n \sum_{k=n}^\infty P(A_k) \geq b_n \infty \implies b_n = 0
.\end{align*}


> Note difference to Borel-Cantelli: that only gives you the equality to zero.


