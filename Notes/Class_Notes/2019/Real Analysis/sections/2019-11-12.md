# Tuesday November 12

## Closed Subspaces and Orthogonal Projections

**Definition:**
Let $H$ be a Hilbert space, then a subspace $M \subseteq H$ is *closed* if $x_n \mapsvia{H} x$ with $\theset{x_n} \subset M$ implies that $x\in M$.

> Note that finite-dimensional subspaces are *always* closed, so this is a purely infinite-dimensional phenomenon.

**Proposition:**
Given any *set* $M$, then 
$$
M^\perp \definedas \theset{x\in H \suchthat \inner{x}{y} = 0 ~\forall y\in M}
$$ 
is always a closed subspace.

*Proof:*
Homework problem.

**Lemma:**
Let $M$ be a closed subspace of $H$ and $x\in H$.
Then

1. There exists a unique $y \in M$ that is *closest* to $y$, i.e. 
$$
\exists y\in M \suchthat \norm{x - y} = \inf_{y' \in M}\norm{x - y'}
.$$

2. Defining $z \definedas x-y$, then $z\in M^\perp$.

**Consequence 1:**
If $M \subseteq H$ is a closed subspace, then $(M^\perp)^\perp = M$.

> Note that $M \subseteq M^{\perp \perp}$ by definition. (Easy to check)

To show that $M^{\perp \perp} \subseteq M$, let $x\in M^{\perp \perp}$, then $x = y + z$ where $y\in M$ and $z\in M^\perp$.

Then 
$$
\inner{x}{z} = \inner{y}{z} + \inner{z}{z} \implies \norm{z}^2 = 0 \implies z = 0 \implies x=y
.$$

**Consequence 2:**

**Theorem:**
If $M \subseteq H$ is a closed subspace, then $H = M \oplus M^\perp$, i.e. 
$$
x\in H \implies x  = y + z, \quad y\in M, ~z\in M^\perp
,$$ 
and $y,z$ are the unique elements in $M, M^\perp$ that are closest to $x$.

*Proof of Lemma (Part 1):*

Let $\delta \definedas \displaystyle\inf_{y' \in M} \norm{x - y'}$, which is a sequence of real numbers that is bounded below, and thus this infimum is attained.
Then there is a sequence $\theset{y_n} \subseteq M$ such that $\norm{x - y_n} \to \delta$.

Consider the following parallelogram:

![Image](figures/2019-11-12-11:25.png)\

Then by the parallelogram theorem, we  have 
$$
2(\norm{y_n - x}^2 + \norm{y_m - x}^2) = \norm{y_n - y_m}^2 + \norm{y_n + y_m - 2x}^2.
$$

which yields

\begin{align*}
\norm{y_n - y_m}^2 
&= 2 \norm{y_n - x}^2 + 2\norm{y_m - x}^2 - 4\norm{\frac 1 2 (y_n + y_m) - x}^2 \\
&\leq 2 \norm{y_n - x}^2 + 2\norm{y_m - x}^2 - 4\delta^2 \to 0,
\end{align*}


since $\norm{y_n - x}_H \to 0$ since $y_n \to_H x$.

It follows that $\theset{y_n}$ is Cauchy in $H$, so $y_n \mapsvia{H} y \in H$. 
But since the $y_n$ were in $M$ and $M$ is closed, we in fact have $y\in M$.
Since $\norm{x - y_n} \to \norm{x - y} = \delta$, we have the existence of $x$.

> We'll establish uniqueness after part 2.

*Proof of Lemma (Part 2):*

Let $u\in M$, we want to show that 
$$
\inner{z}{u} = \inner{x-y}{u} = 0
.$$

Without loss of generality, we can assume that $\inner{z}{u} \in \RR$, since $u$ satisfies this property iff any complex scalar multiple does.

Let $f(t) = \norm{z + tu}^2$ where $t\in \RR$.
Then $f(t) = \norm{z}^2 + zt\inner{z}{y} = t^2 \norm{u}^2$.

We know that $t$ attains a minimum at $t=0$, since $z + tu = x - (y + u)$, but $y$ was the closest element to $x$ and thus the norm is minimized exactly when $z + tu = x - y \implies t=0$.

Because of this fact, we know that $f'(0) = 0$.
But by using Calculus, we can compute that $f'(0) = 2 \inner{z}{u}$, so $\inner{z}{u}$ must equal zero.

Now to show uniqueness, let $y' \in M$ and suppose $y' \neq u$ but $\norm{x-y'} = \delta$.
Then $x- y' = (x-y) + (y-y')$.

But these are two orthogonal terms, so we can apply Pythagoras to obtain

\begin{align*}
\norm{x-y'}^2 &= \norm{x-y}^2 + \norm{y-y'}^2 \\
\implies \delta &= \delta + c \\
\implies c &= 0 \\
\implies \norm{y-y'} &= 0 \\
\implies y &= y'
.\end{align*}


> Note: the statement is the important thing here, less so this particular proof.

## Trigonometric Series

**Theorem:**
Let $e_n(x) \definedas e^{2\pi i n x}$ for all $x\in [0, 1]$ and $n\in \ZZ$.
Then $\theset{e_n}_{n\in \ZZ}$ is an *orthonormal basis* for $L^2([0, 1])$.

> Note: Orthonormality is easily check, so the crux of the proof is showing it's a basis.

> Note: Elements in $\mathrm{span}\theset{e_n}$ are referred to as *trigonometric polynomials*.

Goal:
We'll show that the span of the trigonometric polynomials are dense in $L^2([0, 1])$. 

This will be a consequence of the following theorem:

### Trigonometric Polynomials are Dense in $C^0([0, 1])$

**Theorem (Periodic Analogue of the Weierstrass Approximation Theorem):**
If $f\in C(\Pi)$ (where $\Pi$ is a torus) and $\varepsilon > 0$, then there exists a trigonometric polynomial $P$ such that $\abs{f(x) - P(x)} < \varepsilon$ uniformly for all $x\in \Pi$.

> Note that this measures closes in the *uniform* norm. We can relate these by 
$$
\norm{f(x) -P(x)}_{L^2} \leq \norm{f(x) - P(x)}_{\infty} \text{, i.e. } \int_0^1 \abs{f(x) - P(x)}^2 \leq \sup_x \abs{f(x) - P(x)}^2
.$$

*Proof:*
Identify $\Pi = [- \frac 1 2, \frac 1 2)$.
Suppose there exists a sequence $\theset{Q_k}$ of trigonometric polynomials such that

\begin{align*}
Q_k(x) &\geq 0 \quad \text{ for all } x, k \\
\int_{-1/2}^{1/2} Q_k(x) ~dx &= 1 \quad \forall k \\
\forall \delta>0, \quad Q_k(x) &\mapsvia{u} 0 \text{ uniformly on } \Pi\setminus[-\delta, \delta]
.\end{align*}

> Note that these properties are similar to what we wanted from approximations to the identity.

Define 
$$
P_k(x) = \int_{-1/2}^{1/2} f(y) Q_k(x - y) ~dy
$$ 
by convolving over the circle, then $P_k$ is also a trigonometric polynomial.

We then have

\begin{align*}
I = \abs{ P_k(x) - f(x) } \leq 
\int_{-1/2}^{1/2} \abs{ f(x-y) - f(x) }
Q_k(y) ~dy \quad \text{by Property 2} 
.\end{align*}


We can now note that $f$ is continuous on a compact set, so it is uniformly continuous, and thus for $y$ small enough, we can find a $\delta$ such that
$$
\abs{f(x-y) - f(x)} < \varepsilon/2 \text{ for all } x \in B(\delta, x)
.$$

But this lets us break the integral into two pieces, 
\begin{align*}
I 
&\leq \int_{y \in B_\delta} \abs{ f(x-y) - f(x) }
Q_k(y) ~dy 
+ 
\int_{y \in B_\delta^c} \abs{ f(x-y) - f(x) }
Q_k(y) ~dy \\
&< \int_{y \in B_\delta} \frac \varepsilon 2~
Q_k(y) ~dy 
+
\int_{y \in B_\delta^c} \abs{ f(x-y) - f(x) }
Q_k(y) ~dy \\
&\leq \int_{y \in B_\delta} \frac \varepsilon 2~
Q_k(y) ~dy + \frac \varepsilon 2~ \quad \text{(for $k$ large enough)} \\
&\to 0 \quad \quad \text{since } Q_k \mapsvia{u} 0
.\end{align*}

$\qed$

*Constructing $Q_k$:*

Define
$$
Q_k(x) = c_k \left( \frac {1 + \cos(2\pi x)}{2} \right)^k,
$$

where $c_k$ is chosen to normalize the integral to 1 to satisfy property 2. 
Property 1 is clear, so we just need to show 3,

Since $\cos(x)$ is decreasing on $[\delta, \frac 1 2]$, 
$$
Q_k(x) \leq Q_k(\delta) = c_k \left( \frac{1 + \cos(2\pi \delta)}{2} \right)^k
.$$

Note that the numerator is less than 2, so the entire term is a constant that is less than 1 being raised to the $k$ power. 

So this goes to zero exponentially, the question now depends on the growth of $c_k$.
It turns out that $c_k \leq (k+1)\pi$, so it only grows linearly.
So the whole quantity indeed goes to zero.

We can now write

\begin{align*}
1 &= 2c_k \int_0^{1/2} \left( \frac{1 + \cos(2\pi x)}{2} \right)^k dx \\
&= 2c_k \int_0^{1/2} \left( \frac{1 + \cos(2\pi x)}{2} \right)^k \sin(2\pi x) dx \\
&= \frac{2c_k}{\pi} 
\int_0^1 u^k ~du 
= 
\frac{2c_k}{\pi(k+1)}
.\end{align*}

$\qed$

> Note: this is a nice proof!

Question: when is a function equal to its Fourier series? We have $L^2$ convergence, but when do we get pointwise? 

**Theorem from the 1960s:** any $L^2$ function (in particular continuous functions) converges to its Fourier series *almost everywhere*.

