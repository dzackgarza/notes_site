# Ch. 20 Continued (Friday, May 21)

:::{.remark}
Goal of Ch.18: prove that units theorem where the rank is left unspecified, and we proved it for $g$ the rank of a certain lattice.
For Ch.20, we want to show $g$ is the right number!
Recall the  log function:
\[
\Log: K\units\to \RR^{r_1 + r_2}
\]
where we take the $\log\abs{\wait}$ of the real embeddings and $2\log\abs{\wait}$ of one of each pair of complex embeddings.
We know $\Log\ZZ_K\units \subseteq \RR^{r_1 + r_2}$ is a lattice of rank $g \leq r_1 + r_2 - 1$, and we want to show equality.
We showed that everything follows if we establish the key lemma: if $c_1, \cdots, c_{r_1 + r_2 - 1} \in \RR$ are not all zero, defin
\[
F(\alpha) \da \tv{c_1, \cdots, c_{r_1 + r_2 - 1}, 0} \cdot \Log(\alpha)
.\]
We want to show there is a sequence of $\alpha\in \ZZ_K$ such that $\abs{F(\alpha)} \to \infty$ with $\abs{N( \alpha)} \leq C_1$ bounded, where $C_1$ doesn't depend on $\alpha$.
We want to make the dot product above large while keeping the norms small while making linear combinations of $\log\abs{\sigma_i( \alpha)}$ large.
The only embeddings that show up are the ones labeled up to $r_1 + r_2 - 1$, since the last component is zero, and we'll show that we can essentially control the image of $\alpha$ under these embeddings while keeping its norm bounded.

Recall the Minkowski embedding realizes $K$ as a subspace of Euclidean space: 
\[
\iota: K\to \RR^{r_1} \oplus \CC^{r_2}
,\]
where the first components are the first $r_1$ real embeddings and the latter are the complex embeddings.
Identifying the target with $\RR^n$, we said last time that $\iota(\ZZ_K) \subseteq \RR^n$ is a full rank lattice, so of rank $n$, which thus has a finite nonzero covolume.

We'll use this with Minkowski's convex body theorem: suppose $R$ is a region in $\RR^d$ which is centrally symmetric and convex, and $\Lambda$ is a full rank lattice in $\RR^d$.
Then if $\vol(R)> w^d \covol(\Lambda)$, $R$ contains a nonzero point $x\in \Lambda$.
How we'll use these: apply s the convex body theorem on $\Lambda = \iota(\ZZ_K)$, and use this to locate the types of $\alpha$ we want.
:::

:::{.remark}
Given \( \lambda_1, \cdots, \lambda_{r_1 + r_2} \in \RR^{\geq 0} \), denote this by the vector $\vector \lambda$.
Define
\[
R_{\vector \lambda}\da \ts{ \tv{x_1, \cdots, x_{r_1 + r_2}} \in \RR^{r_1} \oplus \CC^{r_2} \st \abs{x_i} \leq \lambda_i\,\,\forall i  } \subseteq \RR^{r_1} \oplus \CC^{r_2} \cong \RR^{n}
.\]
This is clearly centrally symmetric: negating anything lands in the region since we're taking absolute values.
It's less clearly convex, but this follows from the triangle inequality.
More intuitively, it's a product of discs and boxes, each of which are convex.
One can compute its volume as 
\[
\vol R_{\vector \lambda} = \prod_{i\leq r_1} 2\lambda_i \prod_{j\leq r_2} \pi \lambda_{r_1 + j}^2
= 2^{r_1} \pi^{r_2} \prod_{i\leq r_1} \lambda_i \prod_{j\leq r_2} \lambda_{r_1 + j}^2
,\]
since the first $r_1$ components form boxes and the last form discs.

As a convention, we'll fill out $r_1+r_2$ tuples to $r_1 + 2r_2$ tuples in the following way: for $1 \leq i\leq r_2$,  set 

- $\sigma_{r_1 + r_2 + i} = \bar{\sigma_{r_1 + i}}$, and
- $\lambda_{r_1 + r_2 + i} = \lambda_{r_1 + i}$.

This introduces the remaining embeddings from each conjugate pair and assigns it the same $\lambda_i$.
We can now write
\[
\vol R_{\vector \lambda} = 2^{r_1}\pi^{r_2} \prod_{i\leq n} \lambda_i
,\]
since each $\lambda$ now appears twice in the product.

:::

:::{.proof title="of key lemma"}
Rather than working with arbitrary tuples, we'll only consider $\elts{\lambda}{r_1 + r_2} \in \RR^{\geq 0}$ contained such that
\[
\vol R_{\vector \lambda} = 2 \qty{ 2^n \covol \iota(\ZZ_K) }
,\]
noting that the parenthesized quantity is exactly twice what's needed in Minkowski's theorem.
This allows one to choose the first $r_1 + r_2-1$ freely, and the last will be determined uniquely.
By Minkowski's theorem, there is an $\alpha\in \ZZ_K$ nonzero with $\iota( \alpha)\in R_{\vector \alpha}$.
Hence
\[
\abs{ \sigma_i( \alpha)} \leq \lambda_i && \forall i=1,2,\cdots, n
.\]
Consequently,
\[
\abs{N \alpha} 
&= \prod_{i} \abs{\sigma_i( \alpha)}\leq \prod \lambda_i \\
&= \frac{ \vol R_{\vector \lambda}} {2^{r_1} \pi^{r_2} } \\
&= C_1
.\]
Notice that
\[
1 \leq \abs{N \alpha} 
&= \prod_i \abs{ \sigma_i( \alpha)} \\
&= \abs{ \sigma_j( \alpha)} \prod_{i\neq j} \abs{ \sigma_i( \alpha)} \\
&\leq \abs{ \sigma_j ( \alpha)} \prod_{i\neq g} \lambda_i \\
&\leq { \abs{ \sigma_j } \over \lambda_j } \prod_i \lambda_i \\
&\leq { \abs{ \sigma_j } \over \lambda_j } C_1 
.\]

Rearranging yields
\[
\abs{ \sigma_j ( \alpha)} \geq C_1\inv \lambda_j
.\]
Thus for every $1\leq j \leq n$, we have an inequality
\[
C_1\int \lambda_j \leq \abs{ \sigma_j( \alpha)} \leq \lambda_j
.\]
Taking logarithms yields
\[
\log C_1\inv \leq \log \abs{ \sigma_j( \alpha)} - \log \lambda_j \leq 0
.\]

Recalling the definition of $\Log( \alpha)$, and the above inequality says we know $\log\abs{ \sigma_j( \alpha)}$ up to a constant -- it's about $\log \lambda_j$.
So setting
\[
\vector L \da
\tv{ 
\log \lambda_1, \cdots, \lambda_{r_1}, 2\log \lambda_{r_1 + 1}, 2\log \lambda_{r_1 + r_2}
}
,\]
we have $\vector L \approx \Log \alpha$, where $\approx$ means each component is off by at most a constant.
So $\abs{ \Log \alpha - \vector L} \leq C_2$, some other constant.
We want to understand 
\[ 
F(\alpha) \da \tv{c_1, \cdots, c_{r_1 + r_2 - 1}, 0} \cdot \vector \Log(\alpha)
\approx  
F(\alpha) \da \tv{c_1, \cdots, c_{r_1 + r_2 - 1}, 0} \cdot \vector L
.\] 
How can we estimate the error?
By Cauchy-Schwarz, the difference is bounded by
\[
\norm{
\tv{c_1, \cdots, c_{r_1 + r_2 - 1}, 0} 
}
\cdot 
\norm{
\Log \alpha - \vector L
}
\leq 
\norm{
\tv{c_1, \cdots, c_{r_1 + r_2 - 1}, 0} 
}
C_2
\da 
C_3
.\]

We've show that for any choice of $\lambda_1, \cdots, \lambda_{r_1 + r_2}$ satisfying the volume condition, one can find $\alpha$ where $\abs{N \alpha}$ is bounded and all of the above inequalities hold.
In particular, $F( \alpha) \approx \vector c \cdot \vector L$.
We claim we can now choose $\alpha$s to make $\abs{F( \alpha)} \to \infty$.
Every choice of $\lambda_i$s yield an $\alpha$, and the $c_i$ in $\vector c$ are fixed, so $\vector c \cdot \vector L$ is just a function of the $\lambda_i$.
So to finish the prove, we need to show we can pick $\lambda_i$ satisfying the volume condition but making $\vector c \cdot \vector L$ as large as we want.
This correspondingly makes $F(\alpha)$ large, since they differ by at most a constant.

Here's why we can do this:
we assumed not all $c_i$ were zero, so fix an $i$ where $c_i\neq 0$.
Choose $\lambda_j$ with $\lambda_i$ very large, and $\lambda_{j'} =1$ for $j=1, \cdots, r_1 + r_2 - 1$ not including $i$.
Then choose $\lambda_{r_1 + r_2}$ as determined by the volume condition.
The claim is that we're done.
The dot product has a zeroth $r_1 + r_2$ component, so the dot product doesn't see $\sigma_{r_1 + r_2}$.
Recall $\vector L$ is a function of the $\lambda_i$, since the whole point was that we removed the dependence on $\alpha$, and is given by 
\[
\vector L = \tv{
\log \lambda_1, \cdots, \lambda_{r_1}, 2\log \lambda_{r_1 + 1}, \cdots, 2\log \lambda_{r_1 + r_2}
}
.\]
What survives the dot product?
We're taking $\log(1) = 0$ in the $j\neq i$ components, and in the $i$th component we have at most $2\log \lambda_i \cdot c_i$, where the \( \lambda_i \) is a large number.
So we can make $\vector c \cdot \vector L$ as large as we want.
:::

# Ch. 21: Applications of Minkowski's Theorem

:::{.proposition title="?"}
Recall the Minkowski embedding, $\iota: K\to \RR^{r_1} \oplus \CC^{r_2}$, where we concatenate all of the real embeddings followed by the complex embeddings.
View the target as $\RR^n$, then there is a formula:
\[
\covol \iota(\ZZ_K) = 2^{-r_2} \sqrt{ \abs{ \discriminant_K }}
.\]

:::

:::{.proof title="?"}
Pick an integral basis \( \elts{\omega}{n} \) for $K$, then $\iota(\omega_1), \cdots, \iota(\omega_n)$ span $\iota(\ZZ_K)$ over $\ZZ$.
It suffices to show that if $A$ is the $n\times n$ matrix with $i$th column $\iota(\omega_i)$, then $\abs{\det A}$ is the right-hand side in the formula.
If this is true, the $\iota(w_i)$ are linearly independent over $\RR$, otherwise the determinant would be $\RR$, so they're a basis for the lattice $\iota(\ZZ_K)$, and this determinant is precisely the formula for the covolume of a lattice.
We claim the matrix looks like the following.

\[
\begin{bmatrix}
\sigma_1(\omega_1) & \cdots & \sigma_1(\omega_1) \\
\vdots & \cdots & \vdots \\
\sigma_{r_1}(\omega_1) & \cdots & \sigma_{r_1}(\omega_1) \\
\Re \sigma_{r_1+1}(\omega_1) & \cdots & \Re \sigma_{r_1+1}(\omega_1) \\
\Im \sigma_{r_1+1}(\omega_1) & \cdots & \Im \sigma_{r_1+1}(\omega_1) \\
\vdots & \cdots & \vdots \\
\Re \sigma_{r_1+r_2}(\omega_1) & \cdots & \Re \sigma_{r_1+r_2}(\omega_1) \\
\Im \sigma_{r_1+r_2}(\omega_1) & \cdots & \Im \sigma_{r_1+r_2}(\omega_1) 
\end{bmatrix}
.\]

Let $D_{ij} \da \tv{ \sigma_i(\omega_j) }$, then by tracking row operations that transform $D$ to $A$, $\det(A) = (-i/2)^{r_2} \det(D)$.
So
\[
\abs{\det A} = 2^{-r_2} \abs{\det D}
.\]
Since $\det(D)^2 = \discriminant_K$, then taking square roots yields $\abs{\det D} = \sqrt{ \abs{ \discriminant_K} }$.
:::

:::{.remark}
We now now the covolume of this lattice, we'd like to do this not just for $\ZZ_K$ but also its ideals.
:::

:::{.lemma title="?"}
If $I\normal \ZZ_K$, then $I$ is a free abelian group of rank $n$.
:::

:::{.proof title="?"}
Take any nonzero $\alpha\in I$, then $\alpha \ZZ_K \subseteq I \subseteq \ZZ_K$.
The outer terms are free abelian of rank $n$, so $I$ must be as well.
:::

:::{.proposition title="?"}
If $I\normal \ZZ_K$ is nonzero, then $\iota(I)$ is a full rank lattice in $\RR^n$ and 
\[
\covol \iota(I) = 2^{r_2} \sqrt{\abs {\discriminant_K}} N(I)
.\]
:::

